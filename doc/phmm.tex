\documentclass[a4paper,10pt]{article}

\usepackage[utf8x]{inputenc}
%\usepackage[top=2.35cm, bottom=2.35cm, left=2.60cm, right=2.60cm]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{hyperref} %hyperlinks for references
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{array}
\usepackage{ctable}
\usepackage{url}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{amsthm}
\newtheorem{mydef}{Definition}
\usepackage{tabulary}
\usepackage[labelsep=period,bf]{caption}
\usepackage{subfig}
\usepackage{float}
\usepackage{scalefnt}

% \usepackage[ps2pdf,bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,colorlinks=false,linkcolor=webred]{hyperref}

% \definecolor{webgreen}{rgb}{0, 0.0, 0} % less intense green
% \definecolor{webblue}{rgb}{0, 0, 0.0} % less intense blue
% \definecolor{webred}{rgb}{0, 0, 0} % less intense red

\floatstyle{plain}
\newfloat{heq}{h}{equ}

\usetikzlibrary{arrows,automata}
\tikzstyle{every state}=[fill=white,draw=black,text=black]
\tikzstyle{initial}=[very thick]
\tikzstyle{accepting by double}=[double distance=1pt]

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\plusequal}{\text{+=}}
\DeclareMathOperator{\val}{val}

\SetKwInput{KwComplexityT}{Time Complexity}
\SetKwInput{KwComplexityS}{Space Complexity}



\begin{document}

\section{Preliminaries}
\label{sec:preliminaries}

\begin{mydef}
  A Pair Hidden Markov Model (PHMM) is a tuple $\langle N, S, L, R, A, B, \pi \rangle$ where:
\end{mydef}

\begin{description}
\item $N$: number of states.
\item $S$: finite set of states, $S = \{S_1, S_2, ... , S_N\}$.
\item $L$: finite alphabet of the left stream.
\item $R$: finite alphabet of the right stream.
\item $A$: state transition probability distribution, $A = \{a_{ij}\}$,
  $$a_{ij} = P(S_j | S_i), \quad 1 \leq i,j \leq N$$
\item $B$: state pair observation probability distribution, $B = \{b_i(x, y\}$,
  \begin{flalign*}
    b_i(x, y = P(x, y | S_i),
    & \quad 1 \leq i \leq N \\
    & \quad x \in L \cup \{\epsilon\} \\
    & \quad y \in R \cup \{\epsilon\} \\
    & \quad b_i(\epsilon, \epsilon) = 0 \\
    % we can put the extra limitation that b_i(x, y = 0 if x != y
  \end{flalign*}

\item $\pi$: the initial state distribution, $\pi = \{ \pi_i \}$,
  $$ \pi_i = P(q_1 = S_i), \quad 1 \leq i \leq N$$
\end{description}


For convenience we denote a model's complete parameter set and by extension a model as:
$$\lambda = \langle A, B, \pi \rangle$$

\section{Forward \& Backward Algorithms for PHMMs}

We adapt the classical \emph{forward} and \emph{backward} HMM algorithms to PHMM
as follows. Considering a PHMM model $\lambda$ and an input observation pair
$(\vec{x}, \vec{y}) \in (L, R)$ with lengths of $n$ and $m$, respectively, we
define the alpha probability as:

$$\alpha_i(l, r) = P((x_1 ...\, x_l, y_1 ...\, y_r), S_i|\lambda)$$

i.e., the probability of starting from any state $S_{start}$, and emitting
the symbols $x_1 ...\, x_l$ on the left stream and the symbols $y_1 ...\, y_r$
on the right stream and ending in the state $S_i$, for $1 \leq i, start \leq N$. We
define the complimentary beta probability as:

$$\beta_i(l, r) = P((x_{l+1} ...\, x_n, y_{r+1} ...\, y_m), S_i|\lambda)$$

i.e., the probability of starting from the state $S_i$ and emitting the
symbols $x_{l+1} ...\, x_n$ on the left stream and the symbols $y_{r+1} ...\,
y_m$ on the right stream and ending in any state $S_{end}$, for $1 \leq i, end
\leq N$.

Consequently, the forward algorithm is defined by induction on the alpha
probabilities, see Algorithm \ref{fig:forward}, and analogously the backward
algorithm is defined by induction on the beta probabilities, see Algorithm
\ref{fig:backward}. Note that in HMMs the alpha and beta variables are
categorized by a two-dimensional matrix, whereas in PHMMs they are
three-dimensional matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[]
\caption{forward for PHMMs}
  \DontPrintSemicolon
\KwIn{$\lambda$: PHMM; $(\vec{x}, \vec{y})$: observation pair (of lengths $n$
  and $m$)}
\KwOut{$P((\vec{x}, \vec{y})|\lambda)$; $\alpha$ matrix}
%\KwComplexityT{O(\#transitions T)}
%\KwComplexityS{O(N T)}
\BlankLine

Initialization:
\begin{flalign*}
  & \alpha_j(1, 0) = \pi_j bj(x_1, \epsilon) \\
  & \alpha_j(0, 1) = \pi_j bj(\epsilon, y_1) \\
  & \alpha_j(1, 1) = \pi_j bj(x_1, y_1) + \left[ \displaystyle\sum_{i=1}^N a_{ij} \, \alpha_i(0, 1) \right] b_j(x_1, \epsilon)
  + \left[ \displaystyle\sum_{i=1}^N a_{ij} \, \alpha_i(1, 0) \right] b_j(\epsilon, y_1) \\
  & \alpha_j(0, 0) = \alpha_j(*, -1) = \alpha_j(-1, *) = 0, \\
  & \qquad 1 \leq j \leq N
\end{flalign*}\;

Induction:
\begin{flalign*}
  \alpha_{j}(l, r) =
  &\left[ \displaystyle\sum_{i=1}^N a_{ij} \, \alpha_i(l-1, r-1) \right] b_j(x_l, y_r) + \\
  &\left[ \displaystyle\sum_{i=1}^N a_{ij} \, \alpha_i(l-1, r) \right] b_j(x_l, \epsilon) + \\
  &\left[ \displaystyle\sum_{i=1}^N a_{ij} \, \alpha_i(l, r-1) \right] b_j(\epsilon, y_r), \\
  & \qquad 1 \leq j \leq N \\
  & \qquad 0 \leq l \leq n, \; 0 \leq r \leq m, \; 2 \leq min(l, r)
\end{flalign*}\;

Termination: \;
$$P((\vec{x}, \vec{y})|\lambda) = \displaystyle\sum_{i=j}^N \alpha_j(n, m)$$\;
\label{fig:forward}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{backward for PHMMs}
  \DontPrintSemicolon
\KwIn{$\lambda$: PHMM; $(\vec{x}, \vec{y})$: observation pair (of lengths $n$
  and $m$)}
\KwOut{$\beta$ matrix}
%\KwComplexityT{O(\#transitions T)}
%\KwComplexityS{O(N T)}
\BlankLine

Initialization:
\begin{flalign*}
\beta_{i}(n, m) = 1, \quad  1 \leq i \leq N
\end{flalign*}\;

Induction:
\begin{flalign*}
  \beta_{i}(l, r) = \displaystyle\sum_{j=1}^N a_{ij} \Bigl[
    &\beta_j(l+1, r+1) \; b_j(x_{l+1}, y_{r+1}) \; + \\
    &\beta_j(l+1, r) \; b_j(x_{l+1}, \epsilon) \; + \\
    &\beta_j(l, r+1) \; b_j(\epsilon, y_{r+1}) \Bigr] \\
  &\qquad 1 \leq i \leq N \\
  &\qquad n \geq l \geq 0, \; m \geq r \geq 0, \; 1 \leq max(l, r), \; not ( l
  = n \, \& \, r = m)
\end{flalign*}\;

\label{fig:backward}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Baum-Welch Algorithm for PHMMs}

We define the variable:

$$\xi_{ij}(l, r) = P(q_{(l, r) -1 } = S_i, q_{(l, r)} = S_j | (\vec{x}, \vec{y}), \lambda)$$

i.e., the probability of transitioning from state $S_i$ at time $t-1$ to state
$S_j$ at time $t$, more exactly time $(l, r)$. We can define $\xi$ in terms of
alpha and beta:

\begin{center}
\begin{flalign*}
  \xi_{ij}(l, r) & = \frac{a_{ij} \Bigl[ \alpha_i(l-1, r-1) b_j(x_l, y_r) +
      \alpha_i(l-1, r) b_j(x_l, \epsilon) + \alpha_i(l, r-1) b_j(\epsilon, y_r) \Bigl] \beta_j(l, r)}{P((\vec{x}, \vec{y}) | \lambda)}\\\\
  & = \frac{a_{ij} \Bigl[ \alpha_i(l-1, r-1) b_j(x_l, y_r) +
      \alpha_i(l-1, r) b_j(x_l, \epsilon) + \alpha_i(l, r-1) b_j(\epsilon, y_r) \Bigl] \beta_j(l, r)}
  {\displaystyle\sum_{i=1}^N \displaystyle\sum_{j=1}^N a_{ij} \Bigl[ \alpha_i(l-1, r-1) b_j(x_l, y_r) +
      \alpha_i(l-1, r) b_j(x_l, \epsilon) + \alpha_i(l, r-1) b_j(\epsilon, y_r) \Bigl] \beta_j(l, r)}
\end{flalign*}
\end{center}


We also define the variable:

$$\gamma_i(l, r) = P(q_{(l, r)} = S_i | (\vec{x}, \vec{y}), \lambda) = \displaystyle\sum_{j=1}^N \xi_{ij}(l, r)$$

i.e., the probability of being in state $S_i$ at time $t$, more exactly time
$(l, r)$. We also relate $\gamma$ to alpha and beta:

$$\gamma_i(l, r) = \frac{\alpha_i(l, r) \beta_i(l, r)}{P((\vec{x}, \vec{y}) |
  \lambda)} = \frac{\alpha_i(l, r) \beta_i(l, r)}{\displaystyle\sum
  _{i=1}^N \alpha_i(l, r) \beta_i(l, r)}$$

If we sum these variables over time we have:

$$\displaystyle\sum_{l=1}^{n} \displaystyle\sum_{r=1}^{m} \gamma_i(l, r) = \mbox{ expected number of transitions from $S_i$ }$$
$$\displaystyle\sum_{l=1}^{n} \displaystyle\sum_{r=1}^{m} \xi_{ij}(l, r) = \mbox
{ expected number of transitions from $S_i$ to $S_j$ }$$

We use these variables to obtain the expected number of times each state
transition and state symbol emission is taken, then normalize (maximize) to
reestimate the $\lambda$ parameters:

\begin{center}
\begin{flalign}\label{eq:count_pi}
\bar \pi_j =& \mbox{ expected number of times in state $S_j$ at time $(t=1)$ } = \notag\\
&\gamma_j(1, 0) + \gamma_j(0, 1) + \left[\gamma_j(1, 1) - \sum_{i=1}^{N}
  \xi_{ij}(1, 1)\right]
\end{flalign}
\end{center}

\begin{center}
\begin{flalign}\label{eq:count_a}
\bar a_{ij} =& \frac{\mbox{ expected number of transitions from state $S_i$ to state $S_j$ }}{\mbox{ expected number of
    transitions from state $S_i$ }} = \notag\\
& \frac{\displaystyle\sum_{l=1}^{n} \displaystyle\sum_{r=1}^{m} \xi_{ij}(l, r)}{\displaystyle\sum_{l=1}^{n} \displaystyle\sum_{r=1}^{m} \gamma_i(l, r)}
\end{flalign}
\end{center}

\begin{center}
\begin{flalign}\label{eq:count_b}
\bar b_i(x, y) =& \frac{\mbox{ expected number of times in state $S_i$ and
    observing (x, y)}}{\mbox{ expected number
    of times in state $S_i$ }} = \notag\\
& \frac{\displaystyle\sum_{l=1}^{n}
  \displaystyle\sum_{r=1}^{m} \gamma_i(l, r) \; \mbox{if} \; (l, r) = (x, y)}{\displaystyle\sum_{l=1}^{n} \displaystyle\sum_{r=1}^{m} \gamma_i(l, r)}
\end{flalign}
\end{center}

Consequently, for a list of pair observations $O$, a model $\lambda$, and,
optionally, prior counts for the transitions and observation emissions, we
define the Baum-Welch algorithm for PHMMs in Algorithm \ref{alg:Baum-Welch}.

\vspace{0.4cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{Baum-Welch for PHMMs}
  \DontPrintSemicolon
\KwIn{\\
$\lambda: PHMM$; \\
$O$: list of pair observations;\\
(optionally) $\pi_0, A_0, B_0:$ prior counts}
\BlankLine
\KwOut{$\lambda^*$: trained PHMM}
\BlankLine

Initialization\;
\BlankLine
\Indp Pick arbitrary model parameters ($\bar \lambda = \lambda$)\;
\BlankLine

\Indm Recurrence\;\Indp
\BlankLine
(If given) set $\hat \pi_i,\; \hat a_{ij},\; \hat b_i(l, r)$ to the prior counts\;
For each observation pair $(\vec{x}, \vec{y})$ in $O$:\;\Indp
Calculate $\alpha(l,r)$, $\beta(l,r)$, $P(o | \bar\lambda)$ using the forward
and backward algs.\;
log\_likelihood += $\log P(o | \bar \lambda)$\;
Using equations \ref{eq:count_pi}, \ref{eq:count_a}, and \ref{eq:count_b}, add
expected counts to $\hat \pi_i,\; \hat a_{ij},\; \hat b_i(l, r)$.

\BlankLine
\Indm Normalize over all pair observations to obtain the new parameters:

$$\bar \pi_i = \frac{\hat \pi_i}{\displaystyle\sum_{i=1}^N \hat \pi_i} \qquad
\bar a_{ij} = \frac{\hat a_{ij}}{\displaystyle\sum_{i=1, j=1}^N \hat a_{ij}}
\qquad \bar b_i(l,r) = \frac{\hat b_i(l,r)}{\displaystyle\sum_{l=1, r=1}^{|L|, |R|}
  \hat b_i(l,r)}$$

$$\bar \lambda = \langle \bar A, \bar B, \bar \pi \rangle$$

\Indm Termination\;\Indp
\BlankLine
$\lambda^* = \bar\lambda$, stop if the change in log\_likelihood is less than some predefined threshold or a maximum number of iterations is
exceeded.
\BlankLine
\label{alg:Baum-Welch}
\end{algorithm}


\section{Scaled Forward, Backward, \& Baum-Welch}

The previous algorithms multiply large sequences of probabilities, which end up
escaping a computer's float precision thus leading to underflow errors. To
overcome this problem, the computation of the alpha and beta variables can be
scaled as shown by Rabiner \cite{Rabiner:HMMs}. We adapt here the scaling
procedure for the forward and backward and consequently Baum-Welch algorithms to
PHMMs.

\bibliographystyle{alpha}
{\small
\bibliography{simple}
}

\end{document}
